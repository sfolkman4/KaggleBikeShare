add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
my_glm_model <- linear_reg(penalty=5, mixture=0.5) %>%
set_engine("glmnet")
preg_qf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
predict(preg_qf, new_data = train_data)
view(predict(preg_qf, new_data = train_data))
predict(preg_qf, new_data = train_data)
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
my_glm_model <- linear_reg(penalty=5, mixture=0.5) %>%
set_engine("glmnet")
preg_qf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
# Generate predictions on the train_data
bike_predictions <- predict(preg_qf, new_data = train_data)
# Transform predictions from log scale to original scale (assuming your model uses log scale)
bike_predictions_original <- bike_predictions %>%
mutate(count = exp(.pred)) %>%  # Revert predictions to the original scale
bind_cols(train_data) %>%  # Combine predictions with the original train data to get datetime
select(datetime, count) %>%  # Select only datetime and predicted count columns
mutate(count = pmax(0, count)) %>%  # Ensure no negative values
mutate(datetime = as.character(format(datetime)))  # Convert datetime to character for saving
# Generate predictions on the test_data
bike_test_predictions <- predict(preg_qf, new_data = test_data)
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count))  # Ensure no negative values
mutate(datetime = as.character(format(datetime)))  # Convert datetime to character for saving
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count)) |>  # Ensure no negative values
mutate(datetime = as.character(format(datetime)))  # Convert datetime to character for saving
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count)) |>  # Ensure no negative values
# Write the predictions to a CSV file
vroom_write(x = bike_predictions_original,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count))
# Write the predictions to a CSV file
vroom_write(x = bike_predictions_original,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
library(parsnip)
library(dplyr)
library(vroom)
library(ggplot2)
library(patchwork)
library(tidymodels)
library(parsnip)
library(poissonreg)
train_data <- vroom("C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/train.csv")
test_data <- vroom("C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/test.csv")
train_data <- train_data %>%
select(-casual, -registered)
# Step 2: Change 'count' to log(count) in train_data
train_data <- train_data %>%
mutate(count = log(count))
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
my_glm_model <- linear_reg(penalty=5, mixture=0.5) %>%
set_engine("glmnet")
preg_qf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
# Generate predictions on the test_data
bike_test_predictions <- predict(preg_qf, new_data = test_data)
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count))
# Write the predictions to a CSV file
vroom_write(x = bike_predictions_original,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
# Write the predictions to a CSV file
vroom_write(x = bike_test_predictions_original,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
# Fit the linear model using the transformed training data
my_glm_model <- linear_reg(penalty=1, mixture=0.0001) %>%
set_engine("glmnet")
preg_qf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
# Generate predictions on the test_data
bike_test_predictions <- predict(preg_qf, new_data = test_data)
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count))
# Write the predictions to a CSV file
vroom_write(x = bike_test_predictions_original,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
my_glm_model <- linear_reg(penalty=1, mixture=0.0001) %>%
set_engine("glmnet")
preg_qf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
# Generate predictions on the test_data
bike_test_predictions <- predict(preg_qf, new_data = test_data)
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count))
# Combine predictions with datetime from test_data
bike_test_predictions_with_datetime <- test_data %>%
select(datetime) %>%  # Select the datetime column
bind_cols(bike_test_predictions_original %>% select(predicted_count)) %>%  # Bind predictions
rename(count = predicted_count)  # Rename predicted count to 'count'
# Write the predictions to a CSV file
vroom_write(x = bike_test_predictions_with_datetime,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
# Fit the linear model using the transformed training data
my_glm_model <- linear_reg(penalty=1, mixture=0.0001) %>%
set_engine("glmnet")
preg_qf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_glm_model) |>
fit(data = train_data)
# Generate predictions on the test_data
bike_test_predictions <- predict(preg_qf, new_data = test_data)
# Transform predictions from log scale to original scale (if using log scale)
bike_test_predictions_original <- bike_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%  # Revert predictions to the original scale
mutate(predicted_count = pmax(0, predicted_count))
# Combine predictions with datetime from test_data and format datetime
bike_test_predictions_with_datetime <- test_data %>%
select(datetime) %>%  # Select the datetime column
mutate(datetime = as.character(format(datetime))) %>%  # Format datetime as a character string
bind_cols(bike_test_predictions_original %>% select(predicted_count)) %>%  # Bind predictions
rename(count = predicted_count)  # Rename predicted count to 'count'
# Write the predictions to a CSV file
vroom_write(x = bike_test_predictions_with_datetime,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
# Write the predictions to a CSV file
vroom_write(x = bike_test_predictions_with_datetime,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions.csv",
delim = ",")
#New Model
# Load necessary libraries
library(tidymodels)
library(glmnet)
library(vroom)
# Recipe for transforming the data
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Penalized regression model with tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Set workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model)
# Define grid of tuning parameters
grid_of_tuning_params <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust the levels as needed
# Cross-validation setup
folds <- vfold_cv(train_data, v = 5, repeats = 1)  # 5-fold cross-validation
# Run the tuning process
tuned_results <- tune_grid(
preg_wf,
resamples = folds,
grid = grid_of_tuning_params,
metrics = metric_set(rmse, rsq),
control = control_grid(save_pred = TRUE)
)
# Select the best model based on RMSE
best_model <- select_best(tuned_results, "rmse")
# Recipe for transforming the data
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Penalized regression model with tunable parameters
preg_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Set workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model)
# Define grid of tuning parameters
grid_of_tuning_params <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust the levels as needed
# Cross-validation setup
folds <- vfold_cv(train_data, v = 5, repeats = 1)  # 5-fold cross-validation
# Run the tuning process
tuned_results <- tune_grid(
preg_wf,
resamples = folds,
grid = grid_of_tuning_params,
metrics = metric_set(rmse, rsq),
control = control_grid(save_pred = TRUE)
)
# Select the best model based on RMSE
best_model <- select_best(tuned_results, "rmse")
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
preg_model <- linear_reg(penalty=tune(), mixture=tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(preg_model)
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = L)
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
folds <- vfold_cv(train_data, v=K, repeats = 1)
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
folds <- vfold_cv(train_data, v=5, repeats = 1)
CV_results <- preg_wf %>%
CV_results <- preg_wf |>
CV_results <- preg_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq))
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
preg_model <- linear_reg(penalty=tune(), mixture=tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(preg_model)
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
folds <- vfold_cv(train_data, v=5, repeats = 1)
CV_results <- preg_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq))
View(CV_results)
View(CV_results)
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
select_best(metric = "rmse")
bestTune <- CV_results %>%
select_best(metric = "rmse")
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=trainData)
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_wf %>%
predict(new_data = test_data)
## Predict
new_test <- final_wf %>%
predict(new_data = test_data)
vroom_write(x = new_test,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions1.csv",
delim = ",")
# Since predictions are on the log scale, transform them back to the original scale
new_test_predictions_original <- new_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%   # Reverse the log transformation
mutate(predicted_count = pmax(0, predicted_count))  # Ensure no negative counts
# Since predictions are on the log scale, transform them back to the original scale
new_test_predictions_original <- new_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%   # Reverse the log transformation
mutate(predicted_count = pmax(0, predicted_count))  # Ensure no negative counts
new_test_predictions <- final_wf %>%
predict(new_data = test_data)
# Since predictions are on the log scale, transform them back to the original scale
new_test_predictions_original <- new_test_predictions %>%
mutate(predicted_count = exp(.pred)) %>%   # Reverse the log transformation
mutate(predicted_count = pmax(0, predicted_count))  # Ensure no negative counts
# Combine the predictions with the test data's datetime column
new_test_with_datetime <- test_data %>%
select(datetime) %>%
bind_cols(new_test_predictions_original %>%
select(predicted_count)) %>%  # Select the predicted counts
rename(count = predicted_count)  # Rename the predicted column to 'count'
# Write the final predictions to a CSV file
vroom_write(x = new_test_with_datetime,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions1.csv",
delim = ",")
# Combine the predictions with the test data's datetime column
new_test_with_datetime <- test_data %>%
select(datetime) %>%
mutate(datetime = as.character(format(datetime))) %>%  # Format datetime as a character string
bind_cols(new_test_predictions_original %>%
select(predicted_count)) %>%  # Select the predicted counts
rename(count = predicted_count)  # Rename the predicted column to 'count'
# Write the final predictions to a CSV file
vroom_write(x = new_test_with_datetime,
file = "C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/TestPredictions1.csv",
delim = ",")
install.packages("rpart")
library(rpart)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # What R function to use7
set_mode("regression")
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>% #
set_mode("regression")
min_n=tune() %>%
my_mod <- decision_tree(tree_depth = tune(),
set_engine("rpart") |>
my_mod <- decision_tree(tree_depth = tune(),
min_n=tune() |>
set_mode("regression")
my_mod <- decision_tree(tree_depth = tune(),
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) |>
set_engine("rpart") |>
set_mode("regression")
library(parsnip)
library(dplyr)
library(vroom)
library(ggplot2)
library(patchwork)
library(tidymodels)
library(parsnip)
library(poissonreg)
train_data <- vroom("C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/train.csv")
test_data <- vroom("C:/Users/sfolk/Desktop/STAT348/KaggleBikeShare/test.csv")
library(tidymodels)
library(rpart)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) |>
set_engine("rpart") |>
set_mode("regression")
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) |>
set_engine("rpart") |>
set_mode("regression")
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
# Fit the linear model using the transformed training data
preg_model <- linear_reg(penalty=tune(), mixture=tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(preg_model)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) |>
set_engine("rpart") |>
set_mode("regression")
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
bike_recipe <- recipe(count ~ ., data = train_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather),
season = factor(season)) %>%
step_date(datetime, features = c("dow", "month")) %>%
step_time(datetime, features = c("hour")) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_zv(all_predictors())
preg_wf <- workflow() |>
add_recipe(bike_recipe) |>
add_model(my_mod)
grid_of_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
folds <- vfold_cv(train_data, v=5, repeats = 1)
CV_results <- preg_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq))
grid_of_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
folds <- vfold_cv(train_data, v=5, repeats = 1)
CV_results <- preg_wf |>
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae, rsq))
